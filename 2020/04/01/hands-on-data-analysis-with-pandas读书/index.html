<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>&lt;Hands on Data Analysis With Pandas&gt;读书 - emacsist</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="emacsist" />
  <meta name="description" content="Anaconda 多环境 # 查看所有环境 conda info -e # 查看当前环境详细信息 conda info # 创建" />

  <meta name="keywords" content="Golang, Java, PostgreSQL, Postgres, MySQL, emacsist, RabbitMQ, Go, emacs, orgmode" />






<meta name="generator" content="Hugo 0.57.0" />


<link rel="canonical" href="https://emacsist.github.io/2020/04/01/hands-on-data-analysis-with-pandas%E8%AF%BB%E4%B9%A6/" />

<link href="." rel="alternate" type="application/rss+xml" title="emacsist" />
<link href="." rel="feed" type="application/rss+xml" title="emacsist" />



<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.1.1" rel="stylesheet">





<meta property="og:title" content="&lt;Hands on Data Analysis With Pandas&gt;读书" />
<meta property="og:description" content="Anaconda 多环境 # 查看所有环境 conda info -e # 查看当前环境详细信息 conda info # 创建" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://emacsist.github.io/2020/04/01/hands-on-data-analysis-with-pandas%E8%AF%BB%E4%B9%A6/" />
<meta property="article:published_time" content="2020-04-01T22:57:39+08:00" />
<meta property="article:modified_time" content="2020-04-01T22:57:39+08:00" />
<meta itemprop="name" content="&lt;Hands on Data Analysis With Pandas&gt;读书">
<meta itemprop="description" content="Anaconda 多环境 # 查看所有环境 conda info -e # 查看当前环境详细信息 conda info # 创建">


<meta itemprop="datePublished" content="2020-04-01T22:57:39&#43;08:00" />
<meta itemprop="dateModified" content="2020-04-01T22:57:39&#43;08:00" />
<meta itemprop="wordCount" content="8091">



<meta itemprop="keywords" content="python,pandas,data," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="&lt;Hands on Data Analysis With Pandas&gt;读书"/>
<meta name="twitter:description" content="Anaconda 多环境 # 查看所有环境 conda info -e # 查看当前环境详细信息 conda info # 创建"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">emacsist</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">emacsist</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">&lt;Hands on Data Analysis With Pandas&gt;读书</h1>

      <div class="post-meta">
        <span class="post-time"> 2020-04-01 </span>
        
        <span class="more-meta"> 8091 words </span>
        <span class="more-meta"> 17 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#anaconda-多环境">Anaconda 多环境</a></li>
<li><a href="#准备环境及依赖">准备环境及依赖</a></li>
<li><a href="#数据分析基础及统计学基础">数据分析基础及统计学基础</a>
<ul>
<li><a href="#数据分析">数据分析</a></li>
<li><a href="#统计学基础">统计学基础</a>
<ul>
<li><a href="#概念">概念</a></li>
<li><a href="#descriptive-statistics-描述性统计">descriptive statistics, 描述性统计</a></li>
<li><a href="#inferential-statistics-推论性统计">Inferential statistics , 推论性统计</a></li>
<li><a href="#常见的分布">常见的分布</a>
<ul>
<li><a href="#gaussian-高斯分布">Gaussian 高斯分布</a></li>
<li><a href="#poisson-泊松分布">Poisson 泊松分布</a></li>
<li><a href="#uniform-均匀分布">uniform 均匀分布</a></li>
<li><a href="#常见分布的可视化">常见分布的可视化</a></li>
</ul></li>
<li><a href="#数据缩放-scaling-data">数据缩放 scaling data</a>
<ul>
<li><a href="#min-max-scaling">min-max scaling</a></li>
<li><a href="#standardize-data">standardize data</a></li>
</ul></li>
<li><a href="#量化变量之间的关系">量化变量之间的关系</a>
<ul>
<li><a href="#covariance-协方差-cov">covariance 协方差, COV</a></li>
<li><a href="#pearson-correlation-coefficient">Pearson correlation coefficient</a></li>
</ul></li>
<li><a href="#arima">ARIMA</a></li>
<li><a href="#exponential-smoothing-指数平滑">exponential smoothing, 指数平滑</a></li>
</ul></li>
</ul></li>
<li><a href="#各种常见统计函数">各种常见统计函数</a></li>
<li><a href="#pandas-data-structures">Pandas Data Structures</a>
<ul>
<li><a href="#numpy-风格">NumPy 风格</a></li>
<li><a href="#series">Series</a></li>
<li><a href="#index">Index</a></li>
<li><a href="#dataframe">DataFrame</a></li>
<li><a href="#填充数据到-dataframe">填充数据到 DataFrame</a>
<ul>
<li><a href="#python-object-dataframe">Python Object -&gt; DataFrame</a></li>
<li><a href="#file-dataframe">File -&gt; DataFrame</a></li>
<li><a href="#database-dataframe">Database -&gt; DataFrame</a></li>
<li><a href="#api-dataframe">API -&gt; DataFrame</a></li>
</ul></li>
<li><a href="#inspecting-a-dataframe-object">Inspecting a DataFrame Object</a></li>
<li><a href="#提取数据子集">提取数据子集</a>
<ul>
<li><a href="#selection">Selection</a></li>
<li><a href="#slicing">Slicing</a></li>
<li><a href="#indexing">Indexing</a></li>
<li><a href="#filtering">Filtering</a></li>
</ul></li>
<li><a href="#添加或删除数据">添加或删除数据</a></li>
</ul></li>
<li><a href="#使用-pandas-进行数据分析">使用 Pandas 进行数据分析</a>
<ul>
<li><a href="#data-wrangling">Data Wrangling</a>
<ul>
<li><a href="#cleaning-data">cleaning data</a>
<ul>
<li><a href="#renaming-columns">renaming columns</a></li>
<li><a href="#type-conversion">Type conversion</a></li>
<li><a href="#ordering-reindexing-sorting-data">Ordering, reindexing, sorting data</a></li>
</ul></li>
<li><a href="#restructing-the-data">Restructing the data</a>
<ul>
<li><a href="#transpose-dataframes">Transpose DataFrames</a></li>
<li><a href="#pivoting-dataframes">Pivoting DataFrames</a></li>
<li><a href="#melting-dataframes">Melting DataFrames</a></li>
</ul></li>
<li><a href="#handling-duplicate-missing-or-invalid-data">Handling duplicate, missing, or invalid data</a>
<ul>
<li><a href="#概要查看">概要查看</a></li>
<li><a href="#处理-null-value">处理 <code>null</code> value</a></li>
<li><a href="#处理重复行">处理重复行</a></li>
</ul></li>
<li><a href="#mitigating-the-issues">Mitigating the issues</a>
<ul>
<li><a href="#dropna">dropna</a></li>
<li><a href="#fillna">fillna</a></li>
</ul></li>
</ul></li>
<li><a href="#aggregating-pandas-dataframe">Aggregating Pandas DataFrame</a>
<ul>
<li><a href="#database-style">Database-Style</a>
<ul>
<li><a href="#quering-dataframe">Quering DataFrame</a></li>
<li><a href="#merging-dataframes">Merging DataFrames</a></li>
</ul></li>
<li><a href="#dataframe-operations">DataFrame operations</a>
<ul>
<li><a href="#算术运算">算术运算</a></li>
<li><a href="#binning-and-thresholds">binning and thresholds</a></li>
<li><a href="#applying-function">Applying function</a></li>
<li><a href="#window-calculations">Window calculations</a>
<ul>
<li><a href="#rolling"><code>rolling()</code></a></li>
<li><a href="#expanding"><code>expanding()</code></a></li>
<li><a href="#ewm"><code>ewm()</code></a></li>
</ul></li>
</ul></li>
<li><a href="#aggregations-with-pandas-and-numpy">Aggregations with Pandas and NumPy</a>
<ul>
<li><a href="#summarizing-dataframes">Summarizing DataFrames</a></li>
<li><a href="#using-groupby">Using Groupby</a></li>
<li><a href="#pivot-tables-and-crosstabs">Pivot tables and crosstabs</a></li>
</ul></li>
<li><a href="#time-series">Time series</a>
<ul>
<li><a href="#time-based-selection-and-filtering">Time-based selection and filtering</a></li>
<li><a href="#shifting-for-lagged-data">shifting for lagged data</a></li>
<li><a href="#differenced-data">Differenced data</a></li>
<li><a href="#resampling">Resampling</a></li>
<li><a href="#merging">Merging</a></li>
</ul></li>
</ul></li>
<li><a href="#visualizing-data-with-pandas-and-matplotlib">Visualizing Data with Pandas and Matplotlib</a>
<ul>
<li><a href="#plotting-with-pandas">Plotting with Pandas</a>
<ul>
<li><a href="#relationships-with-variables">Relationships with variables</a></li>
<li><a href="#distribution">Distribution</a>
<ul>
<li><a href="#histograms">histograms</a></li>
<li><a href="#kdes">KDEs</a></li>
<li><a href="#ecdfs">ECDFs</a></li>
<li><a href="#box-plot">box plot</a></li>
</ul></li>
<li><a href="#counts-and-frequencies">Counts and frequencies</a>
<ul>
<li><a href="#bar">bar</a></li>
<li><a href="#hbar">hbar</a></li>
<li><a href="#stack-bar">stack bar</a></li>
<li><a href="#normalized-stacked-bar">normalized stacked bar</a></li>
</ul></li>
</ul></li>
<li><a href="#pandas-plotting-subpackage">Pandas.plotting subpackage</a>
<ul>
<li><a href="#scatter-matrices">scatter matrices</a></li>
<li><a href="#lag-plots">Lag plots</a></li>
<li><a href="#autocorrelations-plots">Autocorrelations plots</a>
<ul>
<li><a href="#bootstrap-plot">Bootstrap plot</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#plotting-with-seaborn">Plotting with Seaborn</a>
<ul>
<li><a href="#utilizing-seaborn-for-advanced-plotting">Utilizing seaborn for advanced plotting</a>
<ul>
<li><a href="#categorical-data">Categorical data</a></li>
<li><a href="#correlations-and-heatmaps">Correlations and heatmaps</a></li>
<li><a href="#regression-plots">Regression plots</a></li>
<li><a href="#distributions">Distributions</a></li>
<li><a href="#faceting">Faceting</a></li>
<li><a href="#formatting">formatting</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#dataframe-与股票市场">DataFrame 与股票市场</a></li>
<li><a href="#参考资料">参考资料</a></li>
</ul>
</nav>
  </div>
</div>

    
    <div class="post-content">
      

<h1 id="anaconda-多环境">Anaconda 多环境</h1>

<pre><code class="language-bash"># 查看所有环境
conda info -e
# 查看当前环境详细信息
conda info

# 创建新环境
conda create -n env_name 
# 创建并指定 python 版本
conda create -n book_env python=3.6.5 --channel conda-forge


# 激活某环境
conda activate env_name
# 退出某环境
conda deactivate

# 删除环境
conda remove -n env_name --all

# 为指定环境安装包
conda install -n env_name pandas
# 查看环境所安装的包
conda list -n env_name

# 删除安装包
conda remove -n env_name numpy
</code></pre>

<h1 id="准备环境及依赖">准备环境及依赖</h1>

<pre><code class="language-bash"># 要安装最新的 gcc 先
brew install gcc
conda create -n book_env python=3.6.5 --channel conda-forge
conda activate book_env
#conda install -c conda-forge imbalanced-learn
conda install pip

# 超时 -default-timeout=1000
pip install -r requirements.txt

# 然后启动
jupyter lab
</code></pre>

<h1 id="数据分析基础及统计学基础">数据分析基础及统计学基础</h1>

<h2 id="数据分析">数据分析</h2>

<ol>
<li>数据收集

<ul>
<li>HTML, 爬虫</li>
<li>API</li>
<li>DB</li>
<li>可供下载网络资源</li>
<li>日志文件等</li>
</ul></li>
<li>数据整理. 可能遇到的问题

<ul>
<li>人为错误的数据</li>
<li>计算错误的数据</li>
<li>异常值</li>
<li>不完整的数据</li>
<li>数据格式问题</li>
</ul></li>
<li>探索性数据分析, EDA (Exploratory data analysis)</li>
<li>得出结论</li>
</ol>

<h2 id="统计学基础">统计学基础</h2>

<h3 id="概念">概念</h3>

<ul>
<li><code>Sample</code> : 样本

<ul>
<li>必须是随机样本. <code>random sample</code> . 即是没偏见的.</li>
<li>取样方法</li>
<li><code>simple random sample</code> : 简单随机抽样</li>
<li><code>stratified random sample</code> : 分层随机抽样(数据有分组)</li>
<li><code>bootstrap sample</code> : 从样本中随机重抽样替换后的样本. 例如, 有个 sample 为 <code>1, 2, 4, 4, 10</code>, 则 bootstrap sample 可能为 <code>2, 1, 10, 4, 2</code> 或 <code>4, 10, 10, 2, 4</code> 或 <code>4, 1, 1, 4, 10</code> 等.</li>
</ul></li>
<li><code>Population</code> : 总体</li>
<li>统计类型

<ul>
<li><code>descriptive statistics</code> : 描述性统计</li>
<li><code>inferential statistics</code> : 推论性统计</li>
</ul></li>
</ul>

<h3 id="descriptive-statistics-描述性统计">descriptive statistics, 描述性统计</h3>

<p>​   下面描述的都是单变量的</p>

<ul>
<li><p><code>central tendency</code> : 集中趋势</p>

<ul>
<li><code>mean</code> : 总体的平均值写为 $\mu$ , 样本的平均值写为 $\bar{x}$ . 公式  $\bar{x} = \frac{\sum<em>{1}^{n} x</em>{i}}{n}$</li>
<li>它对 outlier 值敏感</li>
<li><code>median</code> : 表示数据的 $50^{th}$ 百分位. 即中位数.</li>
<li>这表示 <code>50%</code> 的数据大于该值, <code>50%</code> 的数据小于该值</li>
<li>计算公式: 从一个排序的数据中, 取中间那个</li>
<li><code>mode</code> : 众数</li>
<li><code>unimodal</code> : 单峰</li>
<li><code>bimodal</code> : 双峰</li>
<li><code>multimodal</code> : 多峰</li>
</ul></li>

<li><p><code>Measures of spread</code> : 离散度</p>

<ul>
<li><p><code>range</code> : $range = max(X) - min(X)$</p></li>

<li><p><code>variance</code> : 方差. 总体的方差写为 $\sigma^2$ , 样本的方差写为 $s^2$</p></li>

<li><p>方差是各个数据与其算术平均数的离差平方和的平均数</p></li>

<li><p>如果想用样本方差来估算总体方差, 则除以 <code>n-1</code> 而不是 <code>n</code> , 这叫 <code>Bessel's correction</code></p></li>

<li><p>$s^2 = \frac{\sum_{1}^{n} (x_i - \bar{x})^2}{n-1}$</p></li>

<li><p><code>standard deviation</code> : 标准差.</p></li>

<li><p>由于方差的单位, 与数据的单位不一致, 所以这里转换为一至才可比较.</p></li>

<li><p>$s = \sqrt{\frac{\sum_{1}^{n} (x_i - \bar{x})^2}{n-1}} = \sqrt{s^2}$</p></li>

<li><p>越小, 分布曲线越瘦; 越大, 分布曲线越肥</p></li>

<li><p><code>Coefficient of variation</code>, CV, 变异系数</p></li>

<li><p>标准差与平均值之比. $CV = \frac{s}{\bar{x}}$</p></li>

<li><p>它告诉我们, 标准差相对于平均值有多大</p></li>

<li><p>用于<code>比较不同数据集</code>的<code>离散程度</code></p></li>

<li><p><code>Interquartile range</code> : <code>IQR</code>, 四分位范围</p></li>

<li><p>$50^{th}$ 百分位, 也叫 $2^{nd}$ 四分位数. $Q_2$</p></li>

<li><p>百分位与四分位都叫分位数(<code>quantiles</code>). 百分位给出的是 100 分. 而四分位给出的四个<code>25%(), 50%, 75%, 100%</code></p></li>

<li><p>$Q_1 表示 25\%$, $Q_2 表示 50\%$, $Q_3 表示 75\%$, $Q_4 表示 100\%$</p></li>

<li><p>$IQR = Q_3 - Q_1$</p></li>

<li><p><code>Quartile coefficient of dispersion</code> : <code>QCD</code> 四分位离散系数.</p></li>

<li><p>$QCD = \frac{\frac{Q_3 - Q1}{2}}{\frac{Q_1 + Q3}{2}} = \frac{Q_3 - Q_1}{Q_3 + Q_1}$</p></li>

<li><p>用于比较不同数据集</p></li>

<li><p><code>Summarizing data</code> : 数据摘要. <code>5-number summary</code></p></li>
</ul>

<table>
<thead>
<tr>
<th></th>
<th>quartile</th>
<th>statistic</th>
<th>percentile</th>
</tr>
</thead>

<tbody>
<tr>
<td>1</td>
<td>$Q_0$</td>
<td>minimum</td>
<td>$0^{th}$</td>
</tr>

<tr>
<td>2</td>
<td>$Q_{1}$</td>
<td>N/A</td>
<td>$25^{th}$</td>
</tr>

<tr>
<td>3</td>
<td>$Q_{2}$</td>
<td>Median</td>
<td>$50^{th}$</td>
</tr>

<tr>
<td>4</td>
<td>$Q_{3}$</td>
<td>N/A</td>
<td>$75^{th}$</td>
</tr>

<tr>
<td>5</td>
<td>$Q_{4}$</td>
<td>maximum</td>
<td>$100^{th}$</td>
</tr>
</tbody>
</table>

<ul>
<li><code>box plot</code> : 箱图.  它是 <code>5-number summary</code> 的可视化.</li>
</ul>

<p><img src="/img/image-20200402203552178.png" alt="image-20200402203552178" /></p>

<ul>
<li><p><code>histograms</code> : 直方图, 通常用于离散变量 <code>discrete variables</code></p></li>

<li><p><code>kernel density estimates</code> : KDEs , 内核密度估计. 用于连续变量. <code>continuous variables</code> . 它类似于直方图, 但它不用 <code>bins</code> , 而不画一条平滑曲线, 它表示对于连续型变量分布的概率密度函数(<code>probability density function,</code> PDF)的估计. PDF 值越高, 表示越可能是它. 当存在负偏态(向左倾斜)时, 表示均值 <code>mean</code> 小于中值 <code>median</code> ; 当存在正偏态(向右作料)时表示 <code>mean</code> 值大于中值 <code>median</code>  ; 当没有倾斜时, 两者相等.</p></li>
</ul>

<p><img src="/img/image-20200402205130885.png" alt="image-20200402205130885" /></p>

<p>当我们对获得一个 <code>&lt;=x</code> 的可能性时, 可以用 <code>cumulative distribution function, CDF</code> (累积分布函数), 它是曲线下面区域的积分.</p>

<p>$ CDF = F(x) = \int<em>{-\infty}^{x} f(t)dt$  , $f(t)$ 是 PDF(概率密度函数), 并且 $\int</em>{-\infty}^{\infty}f(t) dt = 1$</p>

<p>对于连续型变量, 准确得到 x 的概率为 0. 这是因为概率为 x 到 x 的积分, 结果为 0 (在曲线区域下面, 并且 0 宽度的意思).</p>

<p>即 $P(X = x) = \int_{x}^{x}f(t)dt = 0$</p>

<p>为了可视化它, 我们可以从样本中找到 CDF 的估计, 称为<code>empirical cumulative distribution function, ECDF</code> , 先验累积分布函数.  例如</p>

<p><img src="/img/image-20200402210556690.png" alt="image-20200402210556690" /></p>

<ul>
<li>对于离散型分布, 我们使用 <code>probability mass function, PMF</code> (概率质量函数), 来代替 <code>probability density function, PDF</code> (概率密度函数)</li>
</ul></li>
</ul>

<h3 id="inferential-statistics-推论性统计">Inferential statistics , 推论性统计</h3>

<p><code>我们无法控制自变量, 意味着我们无法得出因果关系</code></p>

<p>通过实验，我们可以直接影响自变量并将主题随机分配给对照组和测试组，例如A / B测试. 理想的设置应该是 <code>double-blind</code> (双盲)</p>

<p>用来估计总体参数的统计量称为估计量 <code>estimator</code> , 它是一个随机变量. 比如, 用样本平均数 $\bar x$ 去估计总体平均数 $\mu$ , 这里的 $\bar x$ 是 $\mu$ 的估计量. 如果用估计量的单一值作为总体参数的估计值, 那就是点估计 <code>point estimation</code>  . 如果指定估计量的一个取值范围都作为总体参数的估计, 那便称区间估计 <code>interval estimation</code> .</p>

<p><code>confidence coefficient</code> : 置信系数: 所构建的敬意可以包含总体参数的概率. 这个概率越高, 估计的可靠程度越高, 做出决策的把握也就越大. $置信系数 = 1 - \alpha $ , 其中,  $\alpha$ 是显著性水平.</p>

<p><code>condidence intervals</code> : 置信区间. 按照一定置信系数所求得的估计区间. 该区间提供点估计和周围的误差范围. 通常以 <code>95%</code>  作为置信水平. 它是随机变量.</p>

<h3 id="常见的分布">常见的分布</h3>

<p>有一些常见的概率分布</p>

<h4 id="gaussian-高斯分布">Gaussian 高斯分布</h4>

<p>或叫正太分布(<code>normal</code>). 看上去像一个钟形曲线. 它的参数由</p>

<ul>
<li><code>mean</code> ( $\mu$ )平均值</li>
<li><code>standard deviation</code> ($\sigma$) 标准差来决定.</li>
</ul>

<p>标准的正态分布<code>standard normal(Z)</code> 是由 <code>mean</code> 平均值为 0, 以及 <code>standard deviation</code> 标准差为 1组成的. 许多自然界中的东西都呈正态分布.</p>

<h4 id="poisson-泊松分布">Poisson 泊松分布</h4>

<p>它是一种离散分布. 通常用于对到达进行建模.</p>

<p>在到达之间的时间, 可以被指数分布(<code>exponential distribution</code>)进行建模.</p>

<p>二者都可以通过它们的 <code>mean</code> , 以及 <code>lambda</code> ($\lambda$)  来定义.</p>

<h4 id="uniform-均匀分布">uniform 均匀分布</h4>

<p>在它的范围内, 每个值的概率是相等的. 当我们使用一个随机数来模拟单个成功或失败结果时, 这叫 <code>Bernoulli trial</code> (伯努利试验). 它的参数是成功的概率 $p$  . 当我们多次 <code>n</code> 运行相同的实验时, 成功的总数就是一个二项式(<code>binomial</code>)随机变量.</p>

<p><code>Bernoulli</code> 和 <code>Binomial</code> 都是离散分布</p>

<h4 id="常见分布的可视化">常见分布的可视化</h4>

<p><img src="/img/image-20200402221651631.png" alt="image-20200402221651631" /></p>

<h3 id="数据缩放-scaling-data">数据缩放 scaling data</h3>

<h4 id="min-max-scaling">min-max scaling</h4>

<p>$x_{scaled} = \frac{x - min(X)}{range(X)}$</p>

<h4 id="standardize-data">standardize data</h4>

<p>标准化数据</p>

<p>$z_i = \frac{x_i - \bar x}{s}$</p>

<p>即 元素与平均值之差, 再除以标准差. 结果就是著名的 <code>Z-score</code> .</p>

<p>我们得到的均值为 0 且标准差(和方差)为 1 的归一化分布. <code>Z-score</code> 告诉我们每个观察值与均值有多少标准差; 平均值的 <code>Z-score</code> 为 0. 而低于平均值的 <code>0.5</code> 标准差的观察值的 <code>Z-score</code> 为 <code>-0.5</code></p>

<h3 id="量化变量之间的关系">量化变量之间的关系</h3>

<h4 id="covariance-协方差-cov">covariance 协方差, COV</h4>

<p>$$
cov(X, Y) = E[ (X - E[X]) (Y - E[Y]) ]
$$</p>

<p><code>E[X]</code> 表示 X 的期望</p>

<h4 id="pearson-correlation-coefficient">Pearson correlation coefficient</h4>

<p>Pearson 相关系数, 符号为 $\rho$
$$
\rho _{X, Y} = \frac{cov(X, Y)}{s_X s_Y}
$$
这会标准化 COV (协方差) 并且结果为统计范围的 <code>-1 ~ 1</code></p>

<ul>
<li>1 表示完全正相关</li>
<li>-1 表示完全负相关</li>
<li>0 附近表示没有相关</li>
<li><code>绝对值</code>在 1 附近表示强相关</li>
<li><code>绝对值</code> 小于 0.5 表示弱相关</li>
</ul>

<p><img src="/img/image-20200402223740061.png" alt="image-20200402223740061" /></p>

<p><code>注意, 相关性并不等同于因果性</code></p>

<h3 id="arima">ARIMA</h3>

<p>对于<code>时序数据</code>, 我们常见的建模方法有 指数平滑 <code>exponential smoothing</code> , 以及 ARIMA 家族模型</p>

<ul>
<li><code>autoregressive, AR</code> . 自回归.

<ul>
<li>利用了以下事实: 时间 t 的观察与之前的观察相关</li>
<li>注意, 不是所有的时序都是 AR 的</li>
</ul></li>
<li><code>integrated, I</code>. 整合. 它关注差异(<code>differenced data</code>)数据 , 或者数据从一个时间到另一个时间的变化. 比如如果我们关注一个 lag  , 则差异数据的值为 <code>t - 1</code></li>
<li><code>moving average, MA</code> . 移动平均. 它使用 <code>sliding window</code> (滑动窗口) 来计算最近 x 个观察值的平均值, x 是 <code>sliding window</code> 的长度</li>
</ul>

<h3 id="exponential-smoothing-指数平滑">exponential smoothing, 指数平滑</h3>

<p>它允许我们将更多的权重放在最近的数据, 更少的权重放在旧的数据. (相对于我们的预测位置)</p>

<h1 id="各种常见统计函数">各种常见统计函数</h1>

<pre><code class="language-python">import random
import numpy as np
import pandas as pd
random.seed(0)
salaries = [round(random.random()*1000000, -3) for _ in range(100)]

data = pd.Series(salaries)

# 概要
data.describe()

# mean 平均数
data.mean()

# median 中位数
data.median()

# mode 众数
data.mode()

# var 方差
data.var()

# std 标准差
data.std()

# range
[data.min(), data.max(), data.max() - data.min()]

# coefficient of variation, CV
data.std() / data.mean()

# interquartile range. IQR
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1
print(Q3, Q1, IQR)

# quartile coefficent of dispersion, QCD
(Q3 - Q1) / (Q3 + Q1)

# min-max scaling data
(data - data.min()) / (data.max() - data.min())

# standardizing data. 元素与平均值之差, 再除以标准差
(data - data.mean()) / data.std()

# covariance, COV
data.cov(data)

# Pearson correlation coefficient. 相关性. 默认为 pearson
data.corr(data)
data.corr(data, method='pearson')
data.corr(data, method='spearman')
data.corr(data, method='kendall')
</code></pre>

<h1 id="pandas-data-structures">Pandas Data Structures</h1>

<p>主要有 <code>Series</code> 和 <code>DataFrame</code> . 它们都包含另一个 data struct : <code>Index</code> .</p>

<p>注意, Pandas 的 data struct 是构建于 <code>Numpy</code> 之上的.</p>

<p>上面提到的 data struct , 都是通过 Python 的 <code>classes</code> 来创建的, 当创建一个时, 它们称为 <code>objects</code> 或 <code>instances</code> . 所以, 它们有时会使用 <code>object</code> 自身的 <code>method</code> ; 有时会将它们作为参数传递给其他 <code>function</code></p>

<p>所以, Pandas 实质是一个 object 或 instance. 它们有</p>

<ul>
<li><code>method</code></li>
<li><code>attribute</code></li>
</ul>

<h2 id="numpy-风格">NumPy 风格</h2>

<blockquote>
<p>简单还好, 复杂一点就比较笨重了</p>
</blockquote>

<pre><code class="language-python">import numpy as np

data = np.genfromtxt(
    'data/example_data.csv', delimiter=';', 
    names=True, dtype=None, encoding='UTF'
)

data

# 获取第三列的最大数
max([row[3] for row in data])

# 将它变成 dict
array_dict = {}
for i, col in enumerate(data.dtype.names):
    array_dict[col] = np.array([row[i] for row in data])
array_dict

# 获取字典最大值的所有信息
np.array([value[array_dict['mag'].argmax()] for key, value in array_dict.items()])
</code></pre>

<h2 id="series">Series</h2>

<p><code>单一类型</code>的数组( NumPy 的也是). 你可以想像为电子表格的一列</p>

<ul>
<li>它有一个列名</li>

<li><p>以及相同的数据类型</p>

<pre><code class="language-python">import pandas as pd

place = pd.Series(array_dict['place'], name='place')
place
</code></pre></li>
</ul>

<p>这时, 默认它还含有 <code>Index</code> 对象, 对应相应的行号(从 0 开始, 偏移为 1). Series 常见的属性有</p>

<ul>
<li><code>name</code> : Series 对象的名字</li>
<li><code>dtype</code> : Series 对象的数据类型</li>
<li><code>shape</code> : Series 对象里一行数据里的维度(行数)</li>
<li><code>index</code> : Series 对象的 Index Object</li>
<li><code>values</code> : Series 对象的数据作为 NumPy 数组</li>
</ul>

<p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html</a></p>

<h2 id="index">Index</h2>

<p><code>Index</code> class 是 <code>row label</code> (行标签), 可以通过行来选择. 取决于 Index 的类型,  可以是</p>

<ul>
<li>Row Number : <code>RangeIndex(start=0, stop=5, step=1) . 这也是默认的类型.</code></li>
<li>Date</li>
<li>String</li>
</ul>

<p>可以通过 <code>Series.index</code> 来获取 index 对象.</p>

<p>Index 也是建在 NumPy 的 array 之上. 可以通过 <code>Series.index.values</code> 属性来获取 NumPy Array .</p>

<p>Index 常见的属性有</p>

<ul>
<li><code>name</code></li>
<li><code>dtype</code></li>
<li><code>shape</code></li>
<li><code>values</code></li>
<li><code>is_unique</code></li>
</ul>

<p><code>Pandas</code> 中, 两个 <code>Series</code> 之间的算术运算是在 <code>Index 是否匹配</code>之上的. 例如</p>

<pre><code class="language-python">pd.Series(np.linspace(0, 10, 5)) + pd.Series(np.linspace(0, 10, 5), index=pd.Index([1,2,3,4,5]))
</code></pre>

<p>即</p>

<pre><code class="language-python">0     0.0                1     0.0              0     NaN    
1     2.5                2     2.5              1     2.5 
2     5.0          +     3     5.0       =      2     7.5              
3     7.5                4     7.5              3    12.5
4    10.0                5    10.0              4    17.5
dtype: float64           dtype: float64         5     NaN     
                                                dtype: float64                
</code></pre>

<blockquote>
<p>而 NumPy 是基于元素的位置来进行运算的</p>
</blockquote>

<h2 id="dataframe">DataFrame</h2>

<p>它是建于 <code>Series</code> 之上的. 可以想像它是一个电子表格, 有许多列(相对地, <code>Series</code> 是一列), 每一列都有相同的数据类型.</p>

<p>DataFrame 常见的属性有</p>

<ul>
<li><code>dtypes</code></li>
<li><code>shape</code></li>
<li><code>index</code> : DataFrame 的 Index 对象</li>
<li><code>columns</code> : DataFrame 的列名(作为一个 <code>Index</code> 类型对象)</li>
<li><code>values</code> : DataFrame 中的所有值作为 NumPy 数组</li>
</ul>

<p>DataFrame 之间的操作, 是基于 <code>Index</code> 匹配以及 <code>Column</code> 匹配( Series 则只在 <code>Index</code> 匹配) 之上的.</p>

<h2 id="填充数据到-dataframe">填充数据到 DataFrame</h2>

<blockquote>
<p>Series 是 DataFrame 的必要要素, 它是 DataFrame 的某一列</p>
</blockquote>

<h3 id="python-object-dataframe">Python Object -&gt; DataFrame</h3>

<pre><code class="language-python"># Series -&gt; DataFrame
pd.Series(np.linspace(0, 10, num=5), name='hello').to_frame()

# 多行多列
pd.DataFrame(
    {
        'random': np.random.rand(5),
        'text': ['hot', 'warm', 'cool', 'cold', None],
        'truth': [np.random.choice([True, False]) for _ in range(5)]
    }, 
    index=pd.date_range(
        end=datetime.date(2019, 4, 21),
        freq='1D',
        periods=5, 
        name='date'
    )
)

# list dict  -&gt; DataFrame
pd.DataFrame([
    {'mag' : 5.2, 'place' : 'California'},
    {'mag' : 1.2, 'place' : 'Alaska'},
    {'mag' : 0.2, 'place' : 'California'},
])

# list tuple -&gt; DataFrame
list_of_tuples = [(n, n**2, n**3) for n in range(5)]
pd.DataFrame(
    list_of_tuples, 
    columns=['n', 'n_squared', 'n_cubed']
)

# numpy array -&gt; DataFrame
pd.DataFrame(
    np.array([
        [0, 0, 0],
        [1, 1, 1],
        [2, 4, 8],
        [3, 9, 27],
        [4, 16, 64]
    ]), columns=['n', 'n_squared', 'n_cubed']
)
</code></pre>

<h3 id="file-dataframe">File -&gt; DataFrame</h3>

<ul>
<li>CSV 文件:  <code>df = pd.read_csv('data/earthquakes.csv')</code></li>
<li>Excel 文件: <code>pd.read_excel()</code></li>
<li>JSON 文件: <code>pd.read_json()</code></li>
<li>TSV 文件: <code>pd.read_csv(sep='\t')</code></li>
</ul>

<p>常见的参数</p>

<ul>
<li><code>sep</code> : 分割符</li>
<li><code>header</code> : 列名所在的行号. 默认是 <code>infer</code> , 即让 Pandas 自动判断</li>
<li><code>names</code> : 显式指定列名</li>
<li><code>index_col</code> : 作为 index 的列</li>
<li><code>usecols</code> : 只读取哪些列</li>
<li><code>dtype</code> : 为列指定类型</li>
<li><code>converters</code> : 指定列的数据转换函数</li>
<li><code>skiprows</code> : 跳过多少行</li>
<li><code>nrows</code> : 一次读取多少行</li>
<li><code>parse_dates</code> : 自动将包含日期的列转成 <code>datetime object</code></li>
<li><code>chunksize</code> : 分块读取文件</li>
<li><code>compression</code> : 直接读取压缩的文件而不用解压</li>
<li><code>encoding</code> : 指定文件编码</li>
</ul>

<p>将 DataFrame 保存到文件:</p>

<pre><code class="language-python"># 注意, index 的数据默认也是写入的
df.to_csv('outpupt.csv', index=False)
</code></pre>

<h3 id="database-dataframe">Database -&gt; DataFrame</h3>

<pre><code class="language-python"># 写入DB. to_sql(), if_exists='replace' 表示, 如果存在则替换
import sqlite3

with sqlite3.connect('data/quakes.db') as connection:
    pd.read_csv('data/tsunamis.csv').to_sql(
        'tsunamis', connection, index=False, if_exists='replace'
    )
    
# 从 DB 读取
with sqlite3.connect('data/quakes.db') as connection:
    tsunamis = pd.read_sql('SELECT * FROM tsunamis', connection)

</code></pre>

<h3 id="api-dataframe">API -&gt; DataFrame</h3>

<pre><code class="language-python">import datetime
import pandas as pd
import requests

yesterday = datetime.date.today() - datetime.timedelta(days=1)
api = 'https://earthquake.usgs.gov/fdsnws/event/1/query'
payload = {
    'format' : 'geojson',
    'starttime' : yesterday - datetime.timedelta(days=26),
    'endtime' : yesterday
}
response = requests.get(api, params=payload)

# let's make sure the request was OK
print(response.status_code)

earthquake_properties_data = [data['properties'] for data in earthquake_json['features']]
df = pd.DataFrame(earthquake_properties_data)


</code></pre>

<h2 id="inspecting-a-dataframe-object">Inspecting a DataFrame Object</h2>

<blockquote>
<p>DataFrame 的每一列都是一个 Series</p>
</blockquote>

<pre><code class="language-python"># 是否为空(即没数据)
df.empty

# 形状 (nrows, ncols)
df.shape

# 查看前N 行数据. N不写则为 5
df.head(N)

# 查看后 N 行数据. N不写则为5
df.tail(N)

# 查看所有列
df.columns

# 查看数据类型
df.dtypes

# 详细信息
df.info()

# 概要统计信息
df.describe()

# 如果是 object 类型, 则统计信息不像数字统计那样了
# count, unique(表示去重后的个数), top (众数), freq(众数出现的频率)
df.describe(include=np.object)
df.describe(include='all')

# 只看某列概要
df['colName'].describe()
</code></pre>

<p>以下方法对于 Series 和 DataFrame 都适用:</p>

<table>
<thead>
<tr>
<th>method</th>
<th>des</th>
<th>Data type</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>count()</code></td>
<td>非 null 出现的次数</td>
<td>Any</td>
</tr>

<tr>
<td><code>nunique()</code></td>
<td>唯一值的数量</td>
<td>Any</td>
</tr>

<tr>
<td><code>sum()</code></td>
<td></td>
<td>number or boolean</td>
</tr>

<tr>
<td><code>mean()</code></td>
<td></td>
<td>number or boolean</td>
</tr>

<tr>
<td><code>median()</code></td>
<td></td>
<td>number</td>
</tr>

<tr>
<td><code>min()</code></td>
<td></td>
<td>number</td>
</tr>

<tr>
<td><code>idxmin()</code></td>
<td>Index 中的最小值</td>
<td>Number</td>
</tr>

<tr>
<td><code>max()</code></td>
<td></td>
<td>number</td>
</tr>

<tr>
<td><code>idxmax()</code></td>
<td>Index 中的最大值</td>
<td>number</td>
</tr>

<tr>
<td><code>abs()</code></td>
<td></td>
<td>number</td>
</tr>

<tr>
<td><code>std()</code></td>
<td></td>
<td>number</td>
</tr>

<tr>
<td><code>var()</code></td>
<td></td>
<td>number</td>
</tr>

<tr>
<td><code>cov()</code></td>
<td></td>
<td>number</td>
</tr>

<tr>
<td><code>corr()</code></td>
<td></td>
<td>number</td>
</tr>

<tr>
<td><code>quantile()</code></td>
<td>获取指定分位数</td>
<td>Number</td>
</tr>

<tr>
<td><code>cumsum()</code></td>
<td>累积和</td>
<td>number or boolean</td>
</tr>

<tr>
<td><code>cummin()</code></td>
<td>累积最小值</td>
<td>number</td>
</tr>

<tr>
<td><code>cummax()</code></td>
<td>累积最大值</td>
<td>Number</td>
</tr>

<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

<p>Series 有的方法</p>

<table>
<thead>
<tr>
<th>method</th>
<th>des</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>unique()</code></td>
<td>获取唯一值</td>
</tr>

<tr>
<td><code>value_counters()</code></td>
<td>获取频率计数表</td>
</tr>

<tr>
<td><code>mode()</code></td>
<td>获取众数</td>
</tr>
</tbody>
</table>

<p>Index 有的方法</p>

<table>
<thead>
<tr>
<th>method</th>
<th>des</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>argmax()/argmin()</code></td>
<td>查找 index 的最大/最小值位置</td>
</tr>

<tr>
<td><code>contains()</code></td>
<td></td>
</tr>

<tr>
<td><code>equals()</code></td>
<td>跟另一个 Index 对象比较是否相等</td>
</tr>

<tr>
<td><code>isin()</code></td>
<td>给定一些索引值的 list, 并返回 boolean 的数组, 判断index 是否在指定值中</td>
</tr>

<tr>
<td><code>max()/min()</code></td>
<td>查找 Index 的最大/最小值</td>
</tr>

<tr>
<td><code>nunique()</code></td>
<td>唯一值的个数</td>
</tr>

<tr>
<td><code>to_series()</code></td>
<td>从 Index 中生成一个 Series 对象</td>
</tr>

<tr>
<td><code>unique()</code></td>
<td>查找唯一值</td>
</tr>

<tr>
<td><code>value_counts()</code></td>
<td>生成一个唯一值的频率表</td>
</tr>
</tbody>
</table>

<h2 id="提取数据子集">提取数据子集</h2>

<h3 id="selection">Selection</h3>

<blockquote>
<p>获取列. column selection</p>
</blockquote>

<pre><code class="language-python"># 获取一列
df['msg']

# 多列
df[['msg', 'title']]

# 获取符合条件的列. 获取 title, time 列, 以及所有列名以 mag 开头的列
df[
    ['title', 'time']
    + [col for col in df.columns if col.startswith('mag')]
]


</code></pre>

<h3 id="slicing">Slicing</h3>

<blockquote>
<p>获取行. row slicing</p>
</blockquote>

<pre><code class="language-python"># 注意, 行号是从 0 开始. 这里即获取 第 100 到 102 行的数据, 共 3 行
df[100:103]

# 组合列行
df[['time', 'title']][100:103]

</code></pre>

<h3 id="indexing">Indexing</h3>

<ul>
<li><code>loc[]</code> : 使用 <code>label-based</code></li>
<li><code>iloc[]</code> : 使用 <code>integer-based</code></li>
</ul>

<p>所有的 indexing method, 第一个参数是 row indexer, 然后到 column indexer . 例如</p>

<pre><code class="language-python"># 所有行或列, 则写成 :
df.loc[row_indexer, column_indexer]

df.loc[:, 'title']
df.loc[10:15, ['title', 'mag']]

# 通过 integer 来索引, 列也是从 0 开始
df.iloc[10:15, [19, 8]]

df.iloc[10:15, 8:10]
</code></pre>

<p>查找 <code>scalar value</code> (标量值) , 可以用 <code>at[]</code> 以及 <code>iat[]</code></p>

<pre><code class="language-python"># 获取第 10 行, 列为 mag 的值
df.at[10, 'mag']

# 获取第 10 行, 第 8 列的值. (都是从 0 开始)
df.iat[10, 8]
</code></pre>

<h3 id="filtering">Filtering</h3>

<p><code>Boolean masks</code> : 返回与数据相同结构的 shape, 但里面是用 <code>True/False</code> 填充的. 例如</p>

<pre><code class="language-python">df['mag'] &gt; 2

# 通过它可以进行条件选择 df. 
df[df['mag'] &gt; 2]

# loc 也可以处理 boolean masks
df.loc[df['mag'] &gt; 2, ['mag', 'title']]

# 多条件, 要注意括号. &amp;, |
df.loc[(df['mag'] &gt; 2) &amp; (df['alert'] == 'red'), ['mag', 'title']]

# 如果是字符串
df.loc[
    (df.place.str.contains('Alaska')) &amp; (df.alert.notnull()),
    ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']
]

# 数值范围
df.loc[
    df.mag.between(6.5, 7.5),
    ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']
]

# isin
df.loc[
    df.alert.isin(['orange', 'red']),
    ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']
]

# 获取某列最大, 最小值的行
df.loc[
    [df.mag.idxmin(), df.mag.idxmax()],
    ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']
]
</code></pre>

<h2 id="添加或删除数据">添加或删除数据</h2>

<p>添加</p>

<pre><code class="language-python"># 添加新列
df['ones'] = 1
df['mag_negative'] = df.mag &lt; 0


# 按行拼接
pd.concat([tsunami, no_tsunami])
tsunami.append(no_tsunami)

# 按列拼接
additional_columns = pd.read_csv(
    'data/earthquakes.csv', usecols=['tz', 'felt', 'ids', 'time'], index_col='time'
)
pd.concat(
    [df.head(2), additional_columns.head(2)], axis=1
)

# 指定连接方式, join 参数
pd.concat(
    [tsunami.head(2), no_tsunami.head(2).assign(type='earthquake')], join='inner'
)
</code></pre>

<p>删除</p>

<pre><code class="language-python">del df['ones']

# 弹出并移除某列
one = df.pop('ones')

# 直接删除多行或多列. 
# 删除前 2 行. 默认是按行删除的. axis=0
df.drop([0, 1])

# 删除多列
df.drop(columns=['title', 'mag'])
</code></pre>

<blockquote>
<p>默认情况下, drop 会返回一个新的 dataframe, 如果相在原 dataframe 直接修改, 可传一个参数 <code>inplace=True</code></p>
</blockquote>

<h1 id="使用-pandas-进行数据分析">使用 Pandas 进行数据分析</h1>

<h2 id="data-wrangling">Data Wrangling</h2>

<p>它是将输入的数据进行格式化处理, 使我们可以有意义地分析它. 也称为<code>Data manipulation</code> 通常有三个任务(没有固定顺序的, 看情况)</p>

<ul>
<li><p>Data cleaning</p>

<ul>
<li>Renaming</li>
<li>Sorting and reordering</li>
<li>Data type conversions</li>
<li>Deduplicating data</li>
<li>Addressing missing or invalid data</li>
<li>Filtering to the desired subset of data</li>
</ul></li>

<li><p>Data transformation</p></li>
</ul>

<p>数据分 <code>wide format</code> (这个对数据分析和 DB 设计更好)和 <code>long format</code> (灵活性更好). <code>Pandas</code>  期望它的数据是 <code>wide format</code> 以便进行可视化.</p>

<p><img src="/img/image-20200404164014264.png" alt="image-20200404164014264" /></p>

<ul>
<li><p>Data enrichment</p>

<ul>
<li>Adding new columns</li>
<li>Binning</li>
<li>Aggregating</li>
<li>Resampling</li>
</ul></li>
</ul>

<h3 id="cleaning-data">cleaning data</h3>

<h4 id="renaming-columns">renaming columns</h4>

<pre><code class="language-python"># 查看原列名
df.columns

# 重命名列名. 大部分情况 df 返回的是新 dataframe, 所以这里传一个 inplace, 表示在原 df 上直接修改
df.rename(columns={'old_name':'new_name','old_name1':'new_name1'}, inplace=True)

# 转换为大写
df.rename(str.upper, axis='columns').columns
</code></pre>

<h4 id="type-conversion">Type conversion</h4>

<pre><code class="language-python"># 查看原数据类型
df.dtypes

# 转换某列, 例如日期列 date
df.loc[:, 'date'] = pd.to_datetime(df['date'])

# 在读取 CSV 时直接设置
eastern = pd.read_csv(
    'data/nyc_temperatures.csv', index_col='date', parse_dates=True
).tz_localize('EST')
eastern.head()

# 处理日期的多种方法. 转换时区:
eastern.tz_convert('UTC').head()

# 以月份为间隔 yyyy-MM
eastern.to_period('M').index

# yyyy-MM-01
eastern.to_period('M').to_timestamp().index

# 将索引转换为 datetime
df.index = pd.to_datetime(df.index)
</code></pre>

<p>使用 assign 方式</p>

<pre><code class="language-python">new_df = df.assign(
    date=pd.to_datetime(df.date),
    temp_F=(df.temp_C * 9/5) + 32
)
new_df.dtypes
</code></pre>

<p>使用 astype 方式</p>

<pre><code class="language-python">df = df.assign(
    date=pd.to_datetime(df.date),
    temp_C_whole=df.temp_C.astype('int'),
    temp_F=(df.temp_C * 9/5) + 32,
    temp_F_whole=lambda x: x.temp_F.astype('int')
)
df.head()
</code></pre>

<p>category 类型</p>

<pre><code class="language-python">df_with_categories = df.assign(
    station=df.station.astype('category'),
    datatype=df.datatype.astype('category')
)
df_with_categories.dtypes
</code></pre>

<h4 id="ordering-reindexing-sorting-data">Ordering, reindexing, sorting data</h4>

<pre><code class="language-python">#根据某列,  desc 降序
df.sort_values(by='temp_C', ascending=False).head(10)

# 根据多列排序
df.sort_values(by=['temp_C', 'date'], ascending=False).head(10)

# 获取根据某列排序的, 前 N 行
df.nlargest(n=5, columns='temp_C')
</code></pre>

<p>根据 index 来排序</p>

<pre><code class="language-python">df.nlargest(n=5, columns='temp_C')
df.nlargest(n=5, columns='temp_C')
</code></pre>

<p>重新设置 index</p>

<pre><code class="language-python">df[df.datatype == 'TAVG'].head().reset_index()

# 直接在原 df 修改
df.set_index('date', inplace=True)
</code></pre>

<p><code>reindex</code> : 表示在原有的 index 基础上, 以另一个 index 作为对齐基准</p>

<pre><code class="language-python">sp.reindex(
    bitcoin.index, method='ffill'
).head(10)


# 对齐时, 其他字段的数据填充方式
sp_reindexed = sp.reindex(
    bitcoin.index
).assign(
    volume=lambda x: x.volume.fillna(0), # put 0 when market is closed
    close=lambda x: x.close.fillna(method='ffill'), # carry this forward
    # take the closing price if these aren't available
    open=lambda x: np.where(x.open.isnull(), x.close, x.open),
    high=lambda x: np.where(x.high.isnull(), x.close, x.high),
    low=lambda x: np.where(x.low.isnull(), x.close, x.low)
)
sp_reindexed.head(10).assign(
    day_of_week=lambda x: x.index.day_name()
)
</code></pre>

<h3 id="restructing-the-data">Restructing the data</h3>

<h4 id="transpose-dataframes">Transpose DataFrames</h4>

<pre><code class="language-python"># 行列转置
df.head().T
</code></pre>

<h4 id="pivoting-dataframes">Pivoting DataFrames</h4>

<p><code>long format -&gt; wide format</code></p>

<blockquote>
<p>注意, Pivot 方法期待的数据是 single index 的</p>
</blockquote>

<pre><code class="language-python">pivoted_df = long_df.pivot(index='date', columns='datatype', values='temp_C')
pivoted_df.head()

# 多列. 这也称为 Hierarchical index
pivoted_df = long_df.pivot(index='date', columns='datatype', values=['temp_C', 'temp_F'])
# 获取. temp_C 分组下的 TMIN 列
pivoted_df['temp_C']['TMIN']


# 或用 pd 的函数
pd.pivot( index=long_df.date, columns=long_df.datatype, values=long_df.temp_C ).head()
</code></pre>

<p><code>MultiIndex</code></p>

<pre><code class="language-python">multi_index_df = long_df.set_idnex(['date', 'datatype'])
multi_index_df.index
multi_index_df.head()

# 如果想进行 pivot , 则要 unstack()
unstacked_df = multi_index_df.unstack()
</code></pre>

<blockquote>
<p>默认情况下, <code>unstack()</code> 会将 index 的最内层移出到列 columns</p>
</blockquote>

<h4 id="melting-dataframes">Melting DataFrames</h4>

<p><code>wide format -&gt; long format</code> Melting 与 Pivot 的逆操作. 可直接调用 <code>melt()</code> 方法转为 <code>long format</code> . 要指定的参数有</p>

<ul>
<li><code>id_vars</code> : 在 <code>wide format</code> 的数据中, 要唯一标识一行的列</li>
<li><code>value_args</code> : 包含变量的列(即要将这些列从 <code>wide format</code> -&gt; <code>long format</code> )</li>
<li><code>var_name</code> : 可选参数. 变成 <code>long format</code> 后的列名.</li>

<li><p><code>value_name</code> :可选参数. 对应的值的列名.</p>

<pre><code class="language-python">melted_df = wide_df.melt(
id_vars='date',
value_vars=['TMAX', 'TMIN', 'TOBS'],
value_name='temp_C',
var_name='measurement'
)
melted_df.head()
</code></pre></li>
</ul>

<p>与 pivot 的 <code>unstack()</code> 相比, <code>melting data</code> 有个 <code>stack()</code> 方法, 注意它返回的是 <code>Series</code></p>

<pre><code class="language-python">wide_df.set_index('date', inplace=True)
stacked_series = wide_df.stack()
stacked_series.head()

# 将 Series -&gt; DataFrame
stacked_df = stacked_series.to_frame('values')
stacked_df.head()
</code></pre>

<h3 id="handling-duplicate-missing-or-invalid-data">Handling duplicate, missing, or invalid data</h3>

<h4 id="概要查看">概要查看</h4>

<pre><code class="language-python"># 先看个大概
df.head()
df.tail()
df.describe()
df.info()
</code></pre>

<h4 id="处理-null-value">处理 <code>null</code> value</h4>

<pre><code class="language-python">pd.isnull()
pd.isna()

df.isnull()
df.isna()

# 过虑出 null 或 na
contain_nulls = df[
    df.SNOW.isnull() | df.SNWD.isna()\
    | pd.isnull(df.TOBS) | pd.isna(df.WESF)\
    | df.inclement_weather.isna()
]

contain_nulls.head(10)

# 检查某列包含 null 的行数. 必须是调用 isna , isnull , 而不能直接比较
df[df.inclement_weather.isna()].shape[0]
df[df.SNWD.isin([-np.inf, np.inf])].shape[0]
</code></pre>

<h4 id="处理重复行">处理重复行</h4>

<blockquote>
<p><code>df.duplicated()</code></p>

<p>默认的 <code>keep</code> 参数是 <code>first</code> , 如果数据出现多于一次, 则只返回除了重复出现的第一行数据外的数据. (即只会返回有重复的数据, 然后除去重复的第一行数据, 返回其他行的数据)</p>

<p>如果 <code>keep</code> 参数是 <code>False</code> : 则返回所有重复的行的全部数据</p>
</blockquote>

<pre><code class="language-python">df[df.duplicated()].shape[0]

# 指定的列组合是否有重复. 即 duplicated 的第一个参数
df[df.duplicated(['date', 'station'])].shape[0]
</code></pre>

<h3 id="mitigating-the-issues">Mitigating the issues</h3>

<h4 id="dropna">dropna</h4>

<pre><code class="language-python"># 删除包含 na 的行
df.dropna()

# 删除所有列都是 na 的行
df.dropna(how='all')

# 删除指定列都是 na 的行
df.dropna(how='all', subset=['SNOW','SNWD'])
</code></pre>

<h4 id="fillna">fillna</h4>

<blockquote>
<ul>
<li><code>ffill</code> to forward fill</li>
<li><code>bfill</code> to back fill</li>
</ul>
</blockquote>

<pre><code class="language-python">df.fillna(0, inplace=True)

# 填充方式
df_deduped.assign(
    TMAX=lambda x: x.TMAX.replace(5505, np.nan).fillna(method='ffill'),
    TMIN=lambda x: x.TMIN.replace(-40, np.nan).fillna(method='ffill')
).head()

# 以均值填充
df_deduped.assign(
    TMAX=lambda x: x.TMAX.replace(5505, np.nan).fillna(x.TMIN.median()),
    TMIN=lambda x: x.TMIN.replace(-40, np.nan).fillna(x.TMIN.median()),
    # average of TMAX and TMIN
    TOBS=lambda x: x.TOBS.fillna((x.TMAX + x.TMIN) / 2)
).head()

# 另一种是使用 interpolate 插值
df_deduped.assign(
    # make TMAX and TMIN NaN where appropriate
    TMAX=lambda x: x.TMAX.replace(5505, np.nan),
    TMIN=lambda x: x.TMIN.replace(-40, np.nan),
    date=lambda x: pd.to_datetime(x.date)
).set_index('date').reindex(
    pd.date_range('2018-01-01', '2018-12-31', freq='D')
).apply(
    lambda x: x.interpolate()
).tail(10)
</code></pre>

<p><code>interpolate</code></p>

<p>它的方法签名</p>

<pre><code class="language-python">df.interpolate(
    method='linear',
    axis=0,
    limit=None,
    inplace=False,
    limit_direction='forward',
    limit_area=None,
    downcast=None,
    **kwargs,
)
</code></pre>

<p><img src="/img/image-20200405170544214.png" alt="image-20200405170544214" /></p>

<h2 id="aggregating-pandas-dataframe">Aggregating Pandas DataFrame</h2>

<h3 id="database-style">Database-Style</h3>

<h4 id="quering-dataframe">Quering DataFrame</h4>

<blockquote>
<p>可以用的逻辑</p>

<p><code>or</code> 或 <code>and</code> , 注意是小写的</p>

<p>或用符号</p>

<p><code>|</code> 或 <code>&amp;</code></p>
</blockquote>

<pre><code class="language-python">weather = pd.read_csv('data/nyc_weather_2018.csv')

# 等同于 DB 的 
# SELECT * FROM weather WHERE datatype == &quot;SNOW&quot; AND value &gt; 0
snow_data = weather.query('datatype == &quot;SNOW&quot; and value &gt; 0')
snow_data.head()
</code></pre>

<h4 id="merging-dataframes">Merging DataFrames</h4>

<p>Join 的四种类型</p>

<ul>
<li><code>full</code> (outer)</li>
<li><code>left</code></li>
<li><code>right</code></li>

<li><p><code>inner</code> : 这是 dataframe 的默认方式</p>

<pre><code class="language-python"># 如果两个 dataframe 要 join 的列名不同, 则要指定
inner_join = weather.merge(station_info, left_on='station', right_on='id')

# 如果两个dataframe 列名不同, 也可先处理为相同后再 join
weather.merge(station_info.rename(dict(id='station'), axis=1), on='station').sample(5, random_state=0)

# left 和 right
left_join = station_info.merge(weather, left_on='id', right_on='station', how='left')

right_join = weather.merge(station_info, left_on='station', right_on='id', how='right')

# outer, 即 full. indicator 参数表示添加多一列, 表示该行是哪些数据组合成的
outer_join = weather.merge(
station_info[station_info.name.str.contains('NY')], 
left_on='station', right_on='id', how='outer', indicator=True
)


# 指定按索引来 join
valid_station.merge(
station_with_wesf, left_index=True, right_index=True
)

# 两个 dataframe 相同的列, 指定不同的后缀
valid_station.merge(
station_with_wesf, left_index=True, right_index=True, suffixes=('', '_?')
)
</code></pre></li>
</ul>

<p>索引的集合操作</p>

<pre><code class="language-python"># 先在各自的 dataframe 设置为要指定集合列的索引
weather.set_index('station', inplace=True)
station_info.set_index('id', inplace=True)

# 交集
weather.index.intersection(station_info.index)

# 差集
weather.index.difference(station_info.index)

# 并集
weather.index.unique().union(station_info.index)
</code></pre>

<h3 id="dataframe-operations">DataFrame operations</h3>

<h4 id="算术运算">算术运算</h4>

<pre><code class="language-python">fb.assign(
    abs_z_score_volume=lambda x: x.volume.sub(x.volume.mean()).div(x.volume.std()).abs()
).query('abs_z_score_volume &gt; 3')

(fb &gt; 215).any()
</code></pre>

<h4 id="binning-and-thresholds">binning and thresholds</h4>

<p><code>binning</code> 或叫 <code>discretizing</code> (从连续到离散)</p>

<p>binning 的理解:</p>

<p><img src="/img/image-20200405224416477.png" alt="image-20200405224416477" /></p>

<p>Pandas 提供了 <code>pd.cut()</code> 函数来基于 <code>value</code> 进行 <code>binning</code>  (<code>equal-width</code>)</p>

<pre><code class="language-python"># bins=3 表示基于值的大小, 分成 3 份
volume_binned = pd.cut(fb.volume, bins=3, labels=['low', 'med', 'high'])

# 然后获取
fb[volume_binned == 'high'].sort_values(
    'volume', ascending=False
)

# 也可以指定 bins 的序列
ml_df.assign(
    mag_bin=lambda x: pd.cut(x.mag, np.arange(0, 10))
).mag_bin.value_counts()
</code></pre>

<p><code>pd.qcut()</code> 函数基于分位数来进行 <code>binning</code>  (<code>Quantile-based</code>)</p>

<pre><code class="language-python"># q=4, 表示基于值的分位数, 分成 4 等份.  [0, .25, .5, .75, 1.]
volume_qbinned = pd.qcut(fb.volume, q=4, labels=['q1', 'q2', 'q3', 'q4'])
</code></pre>

<h4 id="applying-function">Applying function</h4>

<p>当我们想在 dataframe 中所有的列都执行相同的代码时, 可以用 <code>apply()</code> 方法</p>

<pre><code class="language-python">oct_weather_z_scores = central_park_weather.loc[
    '2018-10', ['TMIN', 'TMAX', 'PRCP']
].apply(lambda x: x.sub(x.mean()).div(x.std()))
oct_weather_z_scores.describe().T
</code></pre>

<p>注意, Pandas 与 NumPy 是设计用于向量操作的. 非向量的操作, 尽可能避免</p>

<h4 id="window-calculations">Window calculations</h4>

<h5 id="rolling"><code>rolling()</code></h5>

<blockquote>
<p>通过 rolling, 它有一个 sliding window 来计算</p>
</blockquote>

<pre><code class="language-python">central_park_weather['2018-10'].assign(
    rolling_PRCP=lambda x: x.PRCP.rolling('3D').sum()
)

central_park_weather['2018-10'].rolling('3D').mean()
</code></pre>

<p>对不同的列, 应用不同的聚合函数</p>

<pre><code class="language-python">central_park_weather['2018-10-01':'2018-10-07'].rolling('3D').agg(
    {'TMAX': 'max', 'TMIN': 'min', 'AWND': 'mean', 'PRCP': 'sum'}
)
</code></pre>

<h5 id="expanding"><code>expanding()</code></h5>

<blockquote>
<p>表示累积计算. 即表示以当前点数据及之前的所有数据来进行相应的函数计算</p>
</blockquote>

<pre><code class="language-python">central_park_weather['2018-10-01':'2018-10-07'].expanding().agg(
    {'TMAX': np.max, 'TMIN': np.min, 'AWND': np.mean, 'PRCP': np.sum}
)
</code></pre>

<h5 id="ewm"><code>ewm()</code></h5>

<blockquote>
<p>指数加权移动函数.</p>

<p>备注: 这个还没弄懂&hellip;.</p>
</blockquote>

<pre><code class="language-python">fb.assign(
    close_ewma=lambda x: x.close.ewm(span=5).mean()
)
</code></pre>

<h3 id="aggregations-with-pandas-and-numpy">Aggregations with Pandas and NumPy</h3>

<p>设置 Pandas 显示格式</p>

<pre><code class="language-python">pd.set_option('display.float_format', lambda x: '%.2f' % x)
</code></pre>

<h4 id="summarizing-dataframes">Summarizing DataFrames</h4>

<p>直接在 DataFrame 上执行聚合</p>

<pre><code class="language-python">fb.agg({
    'open': np.mean, 
    'high': np.max, 
    'low': np.min, 
    'close': np.mean, 
    'volume': np.sum
})
</code></pre>

<p>也可以在某列上执行多个聚合函数</p>

<pre><code class="language-python">fb.agg({
    'open': 'mean',
    'high': ['min', 'max'],
    'low': ['min', 'max'],
    'close': 'mean'
})
</code></pre>

<h4 id="using-groupby">Using Groupby</h4>

<pre><code class="language-python">fb.groupby('trading_volume').mean()

fb.groupby('trading_volume')['close'].agg(['min', 'max', 'mean'])

fb.groupby('trading_volume').agg({
    'open': 'mean',
    'high': ['min', 'max'],
    'low': ['min', 'max'],
    'close': 'mean'
})
</code></pre>

<h4 id="pivot-tables-and-crosstabs">Pivot tables and crosstabs</h4>

<pre><code class="language-python"># 指定哪一列进行 group , 默认的聚合函数是 average
fb.pivot_table(columns='trading_volume')

# 这个与上面的是转置关系
fb.pivot_table(index='trading_volume')
</code></pre>

<p>在 <code>pivot()</code> 中, 我们不能处理多级目录或重复值目录, 但可用 <code>pivot_table</code> 来解决</p>

<pre><code class="language-python">weather.reset_index().pivot_table(
    index=['date', 'station', 'station_name'], 
    columns='datatype', 
    values='value',
    aggfunc='median'
).reset_index().tail()
</code></pre>

<p>可以用 <code>pd.crosstabs()</code> 函数来获取一个频率表  <code>frequency table</code></p>

<pre><code class="language-python">pd.crosstab(
    index=fb.trading_volume,
    columns=fb.index.month,
    colnames=['month'] # name the columns index
)


pd.crosstab(
    index=fb.trading_volume,
    columns=fb.index.month,
    colnames=['month'],
    normalize='columns'
)

pd.crosstab(
    index=fb.trading_volume,
    columns=fb.index.month,
    colnames=['month'],
    values=fb.close,
    aggfunc=np.mean
)
</code></pre>

<h3 id="time-series">Time series</h3>

<p>当处理时序时, 我们应该将 index 设置为 date 或 datetime 列. 建议使用 <code>DatetimeIndex</code> 类型</p>

<pre><code class="language-python">fb = pd.read_csv(
    'data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True
)
</code></pre>

<h4 id="time-based-selection-and-filtering">Time-based selection and filtering</h4>

<pre><code class="language-python"># 根据年来获取
fb['2018']

# 根据年月来获取
fb['2018-10']

# 根据季度来获取. 等同于 fb['2018-01':'2018-03']
fb['2018-q1']

# 获取第一周的数据
fb.first('1W')

# 获取最后一周的数据
fb.last('1W')

# 根据时间来获取
stock_data_per_minute.at_time('9:30')
stock_data_per_minute.between_time('15:59', '16:00')
</code></pre>

<h4 id="shifting-for-lagged-data">shifting for lagged data</h4>

<pre><code class="language-python">fb.assign(
    prior_close=lambda x: x.close.shift(),
    after_hours_change_in_price=lambda x: x.open - x.prior_close,
    abs_change=lambda x: x.after_hours_change_in_price.abs()
).nlargest(5, 'abs_change')

# 获取某天或离某天最近的数据
fb.asof('2018-09-30')
</code></pre>

<h4 id="differenced-data">Differenced data</h4>

<pre><code class="language-python">fb.drop(columns='trading_volume').diff().head()

fb.drop(columns='trading_volume').diff(-3).head()
</code></pre>

<p>$$
diff = x_{t+N} - x_t
$$</p>

<p>N 是 diff 的参数. 默认是 1</p>

<h4 id="resampling">Resampling</h4>

<p>假设有个按分钟级别的股票数据, 这时可进行 resampling , 如按天</p>

<pre><code class="language-python">stock_data_per_minute.resample('1D').agg({
    'open': 'first',
    'high': 'max', 
    'low': 'min', 
    'close': 'last', 
    'volume': 'sum'
})
</code></pre>

<p>也可其他</p>

<pre><code class="language-python"># 按季度
fb.resample('Q').mean()
</code></pre>

<h4 id="merging">Merging</h4>

<p>当有两分不同粒度的数据时, 有两种不同的 merge 函数</p>

<ul>
<li><code>pd.merge_asof()</code> : 表示按最近匹配的来 merge</li>

<li><p><code>pd.merge_ordered()</code> : 按匹配的 key 来 merge, 没有匹配的则排序</p>

<pre><code class="language-python"># 这种类似 left join
pd.merge_asof(
fb_prices, aapl_prices, 
left_index=True, right_index=True, # datetimes are in the index
# merge with nearest minute
direction='nearest', tolerance=pd.Timedelta(30, unit='s')
).head()

# 这种类似 out (full) join
pd.merge_ordered(
fb_prices.reset_index(), aapl_prices.reset_index(),
fill_method='ffill'
).set_index('date').head()
# 指定 na 数据的填充方式
pd.merge_ordered(
fb_prices.reset_index(), aapl_prices.reset_index(),
fill_method='ffill'
).set_index('date').head()
</code></pre></li>
</ul>

<h2 id="visualizing-data-with-pandas-and-matplotlib">Visualizing Data with Pandas and Matplotlib</h2>

<pre><code class="language-python"># line
fb = pd.read_csv(
    'data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True
)
plt.plot(fb.index, fb.open)
plt.show()

# scatter
plt.plot('high', 'low', 'ro', data=fb.head(20))

#  histograms
plt.hist(quakes.query('magType == &quot;ml&quot;').mag)

# 多子图
x = quakes.query('magType == &quot;ml&quot;').mag
fig, axes = plt.subplots(1, 2, figsize=(10, 3))
for ax, bins in zip(axes, [7, 35]):
    ax.hist(x, bins=bins)
    ax.set_title(f'bins param: {bins}')
</code></pre>

<h3 id="plotting-with-pandas">Plotting with Pandas</h3>

<p>Series 与 DataFrame 都有 <code>plot()</code> 方法. 它的参数如下</p>

<pre><code class="language-python">.plot(
    x=None,
    y=None,
    kind='line',
    ax=None,
    subplots=False,
    sharex=None,
    sharey=False,
    layout=None,
    figsize=None,
    use_index=True,
    title=None,
    grid=None,
    legend=True,
    style=None,
    logx=False,
    logy=False,
    loglog=False,
    xticks=None,
    yticks=None,
    xlim=None,
    ylim=None,
    rot=None,
    fontsize=None,
    colormap=None,
    table=False,
    yerr=None,
    xerr=None,
    secondary_y=False,
    sort_columns=False,
    **kwds,
)
</code></pre>

<ul>
<li>默认情况下 x 轴的数据是 index 对象</li>
<li>要画多个 y 轴的数据, 可以用 <code>y=['col1', 'col2']</code>, 如果不指定, 则默认就显示所有列的数据</li>
<li>要将每列的数据分成不同的图形, 可设置 <code>subplots=True</code></li>

<li><p>要共享坐标轴, 则设置相应的 <code>sharex</code> 和 <code>sharey</code></p>

<pre><code class="language-python">fb.iloc[:5,].plot(
y=['open', 'high', 'low', 'close'],
style=['b-o', 'r--', 'k:', 'g-.'],
title='Facebook OHLC Prices during 1st Week of Trading 2018'
)
</code></pre></li>
</ul>

<h4 id="relationships-with-variables">Relationships with variables</h4>

<pre><code class="language-python">fb.assign(
    max_abs_change=fb.high - fb.low
).plot(
    kind='scatter', x='volume', y='max_abs_change',
    title='Facebook Daily High - Low vs. log(Volume Traded)', 
    logx=True
)
</code></pre>

<h4 id="distribution">Distribution</h4>

<ul>
<li>histograms</li>
<li>kernel density estimates, KDEs</li>
<li>box plots</li>
<li>empirical cumulative distribution functions, ECDFs</li>
</ul>

<h5 id="histograms">histograms</h5>

<pre><code class="language-python">fb.volume.plot(
    kind='hist', 
    title='Histogram of Daily Volume Traded in Facebook Stock'
)

# 在同一个 坐标上画不同类型的数据
fig, axes = plt.subplots(figsize=(8, 5))
for magtype in quakes.magType.unique():
    data = quakes.query(f'magType == &quot;{magtype}&quot;').mag
    if not data.empty:
        data.plot(
            kind='hist', ax=axes, alpha=0.4, 
            label=magtype, legend=True,
            title='Comparing histograms of earthquake magnitude by magType'
        )
plt.xlabel('magnitude') # label the x-axis (discussed in chapter 6)
</code></pre>

<h5 id="kdes">KDEs</h5>

<pre><code class="language-python">fb.high.plot(
    kind='kde', 
    title='KDE of Daily High Price for Facebook Stock'
)
plt.xlabel('Price ($)')

# 与 histogram 一起
ax = fb.high.plot(kind='hist', density=True, alpha=0.5)
fb.high.plot(
    ax=ax, kind='kde', color='blue', 
    title='Distribution of Facebook Stock\'s Daily High Price in 2018'
)
plt.xlabel('Price ($)') # label the x-axis (discussed in chapter 6)
</code></pre>

<h5 id="ecdfs">ECDFs</h5>

<pre><code class="language-python">from statsmodels.distributions.empirical_distribution import ECDF

ecdf = ECDF(quakes.query('magType == &quot;ml&quot;').mag)
plt.plot(ecdf.x, ecdf.y)

# axis labels (we will cover this in chapter 6)
plt.xlabel('mag') # add x-axis label 
plt.ylabel('cumulative probability') # add y-axis label

# add title (we will cover this in chapter 6)
plt.title('ECDF of earthquake magnitude with magType ml')
</code></pre>

<h5 id="box-plot">box plot</h5>

<pre><code class="language-python">fb.iloc[:,:4].plot(kind='box', title='Facebook OHLC Prices Boxplot')
plt.ylabel('price ($)') # label the x-axis (discussed in chapter 6)
</code></pre>

<h4 id="counts-and-frequencies">Counts and frequencies</h4>

<h5 id="bar">bar</h5>

<pre><code class="language-python">fb['2018-02':'2018-08'].assign(
    month=lambda x: x.index.month
).groupby('month').sum().volume.plot.bar(
    color='green', rot=0, title='Volume Traded'
)
plt.ylabel('volume') # label the y-axis (discussed in chapter 6)
</code></pre>

<h5 id="hbar">hbar</h5>

<pre><code class="language-python">quakes.parsed_place.value_counts().iloc[14::-1,].plot(
    kind='barh', figsize=(10, 5),
    title='Top 15 Places for Earthquakes '\
        '(September 18, 2018 - October 13, 2018)'
)
plt.xlabel('earthquakes') # label the x-axis (discussed in chapter 6)
</code></pre>

<h5 id="stack-bar">stack bar</h5>

<pre><code class="language-python">pivot = quakes.assign(
    mag_bin=lambda x: np.floor(x.mag)
).pivot_table(
    index='mag_bin', columns='magType', values='mag', aggfunc='count'
)
pivot.plot.bar(
    stacked=True, rot=0, 
    title='Earthquakes by integer magnitude and magType'
)
plt.ylabel('earthquakes') # label the axes (discussed in chapter 6)
</code></pre>

<h5 id="normalized-stacked-bar">normalized stacked bar</h5>

<pre><code class="language-python">normalized_pivot = pivot.fillna(0).apply(lambda x: x/x.sum(), axis=1)
ax = normalized_pivot.plot.bar(
    stacked=True, rot=0, figsize=(10, 5),
    title='Percentage of earthquakes by integer magnitude for each magType'
)
ax.legend(bbox_to_anchor=(1, 0.8)) # move legend to the right of the plot
plt.ylabel('percentage') # label the axes (discussed in chapter 6)
</code></pre>

<h3 id="pandas-plotting-subpackage">Pandas.plotting subpackage</h3>

<h4 id="scatter-matrices">scatter matrices</h4>

<pre><code class="language-python">from pandas.plotting import scatter_matrix
scatter_matrix(fb, figsize=(10, 10))

# KDE
scatter_matrix(fb, figsize=(10, 10))
</code></pre>

<h4 id="lag-plots">Lag plots</h4>

<p>创建一个<code>data[:-1]</code>（除了最后一个条目之外的所有条目）和<code>data[1:]</code>（从第二个条目到最后一个条目）的散点图</p>

<pre><code class="language-python">lag_plot(fb.close, lag=5)
</code></pre>

<h4 id="autocorrelations-plots">Autocorrelations plots</h4>

<p><code>自相关</code>是指时间序列与滞后版本的自身相关. 可通过函数 <code>autocorrelation_plot()</code> 来发现.</p>

<pre><code class="language-python">from pandas.plotting import autocorrelation_plot


autocorrelation_plot(fb.close)
</code></pre>

<h5 id="bootstrap-plot">Bootstrap plot</h5>

<p>使用 bootstrap sample 的方式来统计 <code>mean</code>, <code>median</code> 和 <code>mid-range</code></p>

<pre><code class="language-python">from pandas.plotting import bootstrap_plot

fig = bootstrap_plot(fb.volume, fig=plt.figure(figsize=(10, 6)))
</code></pre>

<h2 id="plotting-with-seaborn">Plotting with Seaborn</h2>

<blockquote>
<p>Matplotlib 通常处理 wide format data</p>

<p>Seaborn 可同样处理 wide format data 和  long format data</p>
</blockquote>

<h3 id="utilizing-seaborn-for-advanced-plotting">Utilizing seaborn for advanced plotting</h3>

<h4 id="categorical-data">Categorical data</h4>

<p>有两种图形</p>

<ul>
<li><code>stripplot()</code></li>

<li><p><code>swarmplot()</code></p>

<pre><code class="language-python">sns.stripplot(
x='magType',
y='mag',
hue='tsunami',
data=quakes.query('parsed_place == &quot;Indonesia&quot;')
)

sns.swarmplot(
x='magType',
y='mag',
hue='tsunami',
data=quakes.query('parsed_place == &quot;Indonesia&quot;')
)
</code></pre></li>

<li><p><code>data</code> 表示数据源</p></li>

<li><p><code>x</code> : x 坐标的列</p></li>

<li><p><code>y</code> : y 坐标的列</p></li>

<li><p><code>hue</code> : 用于不同颜色的列</p></li>
</ul>

<h4 id="correlations-and-heatmaps">Correlations and heatmaps</h4>

<pre><code class="language-python">sns.heatmap(
    fb.sort_index().assign(
    log_volume=np.log(fb.volume),
    max_abs_change=fb.high - fb.low
).corr(),
    annot=True, center=0
)
</code></pre>

<p>用于代替  <code>scatter matrix 的</code>parplot``</p>

<pre><code class="language-python">sns.pairplot(fb)

sns.pairplot(
    fb.assign(quarter=lambda x: x.index.quarter),
    diag_kind='kde',
    hue='quarter'
)
</code></pre>

<p>如果只想比较两个变量, 可用 <code>joinplot()</code></p>

<pre><code class="language-python">sns.jointplot(
    x='volume',
    y='max_abs_change',
    data=fb.assign(
        volume=np.log(fb.volume),
        max_abs_change=fb.high - fb.low
    )
)

# 并画回归线
sns.jointplot(
    x='volume',
    y='max_abs_change',
    kind='reg',
    data=fb.assign(
        volume=np.log(fb.volume),
        max_abs_change=fb.high - fb.low
    )
)

# KDE
sns.jointplot(
    x='volume',
    y='max_abs_change',
    kind='reg',
    data=fb.assign(
        volume=np.log(fb.volume),
        max_abs_change=fb.high - fb.low
    )
)
</code></pre>

<h4 id="regression-plots">Regression plots</h4>

<pre><code class="language-python"># 按季度画
sns.lmplot(
    x='volume',
    y='max_abs_change',
    data=fb.assign(
        volume=np.log(fb.volume),
        max_abs_change=fb.high - fb.low,
        quarter=lambda x: x.index.quarter
    ),
    col='quarter'
)
</code></pre>

<ul>
<li><code>col</code> : 进行子画图的类型, 即 这里有多少种就有多少个子图</li>
</ul>

<h4 id="distributions">Distributions</h4>

<p>Seaborn 风格的 box plot , 它有额外的分位显示</p>

<pre><code class="language-python">sns.boxenplot(
    x='magType', y='mag', data=quakes[['magType', 'mag']]
)
plt.suptitle('Comparing earthquake magnitude by magType')
</code></pre>

<p><code>violinplot()</code> : 它由 box plots 以及 KDEs 组成</p>

<pre><code class="language-python">fig, axes = plt.subplots(figsize=(10, 5))
sns.violinplot(
    x='magType', y='mag', data=quakes[['magType', 'mag']],  
    ax=axes, scale='width' # all violins have same width
)
plt.suptitle('Comparing earthquake magnitude by magType')
</code></pre>

<h4 id="faceting">Faceting</h4>

<pre><code class="language-python">g = sns.FacetGrid(
    quakes[
        (quakes.parsed_place.isin([
            'California', 'Alaska', 'Hawaii'
        ]))\
        &amp; (quakes.magType.isin(['ml', 'md']))
    ],
    row='magType',
    col='parsed_place'
)
g = g.map(plt.hist, 'mag')
</code></pre>

<h4 id="formatting">formatting</h4>

<p>参考另篇 blog <a href="[/2020/03/16/matplotlib%E5%AD%A6%E4%B9%A0/](https://emacsist.github.io/2020/03/16/matplotlib学习/)">Matplotlib学习</a></p>

<h1 id="dataframe-与股票市场">DataFrame 与股票市场</h1>

<pre><code class="language-python"># pip install mplfinance
# 假设df 的列格式为 Open', 'High', 'Low', 'Close', 'Volume', 'Ticker'
# df 的 index 为 datetime

import mplfinance as mpf

# 将列转换为首字母大写
df.rename(str.capitalize, axis='columns', inplace=True)

# 默认为 OHLC. 
# 允许的类型有 'candle','candlestick','ohlc','bars','ohlc_bars','line'
mpf.plot(df[df['Ticker'] == 'APPL'].drop(columns=['Ticker']),type='ohlc',mav=(3,6,9), volume=True)
</code></pre>

<h1 id="参考资料">参考资料</h1>

<ul>
<li><a href="https://www.statisticshowto.datasciencecentral.com/bootstrap-sample/">https://www.statisticshowto.datasciencecentral.com/bootstrap-sample/</a></li>
<li><a href="https://www.bilibili.com/video/BV1q7411K7fc?from=search&amp;seid=13222970873463015957">https://www.bilibili.com/video/BV1q7411K7fc?from=search&amp;seid=13222970873463015957</a></li>
<li><a href="https://www.bilibili.com/video/BV17W411s7Y5/?spm_id_from=333.788.videocard.0">https://www.bilibili.com/video/BV17W411s7Y5/?spm_id_from=333.788.videocard.0</a></li>
</ul>

    </div>

    
    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">emacsist</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">2020-04-01</span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>

    
    
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">Reward</label>
  <div class="qr-code">
    
    
      <label class="qr-code-image" for="reward">
        <img class="image" src="/img/wxpay.jpeg">
        <span>wechat</span>
      </label>
    
      <label class="qr-code-image" for="reward">
        <img class="image" src="/img/alipay.jpeg">
        <span>alipay</span>
      </label>
  </div>
</div>

    <footer class="post-footer">
      <div class="post-tags">
          
          <a href="/tags/python/">python</a>
          
          <a href="/tags/pandas/">pandas</a>
          
          <a href="/tags/data/">data</a>
          
        </div>

      
      <nav class="post-nav">
        
        
          <a class="next" href="https://emacsist.github.io/2020/03/30/logback%E6%9D%82%E9%A1%B9/">
            <span class="next-text nav-default">Logback杂项</span>
            <span class="prev-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  <div id="comments-gitment"></div>
  <link rel="stylesheet" href="/lib/gitment/gitment-0.0.3.min.css">
    <script src="/lib/gitment/gitment-0.0.3.min.js"></script>
  <script type="text/javascript">
  const gitment = new Gitment({
    id: '2020-04-01 22:57:39 \x2b0800 CST',
    title: '\x3cHands on Data Analysis With Pandas\x3e读书',
    link: decodeURI(location.href),
    desc: 'Anaconda 多环境 # 查看所有环境 conda info -e # 查看当前环境详细信息 conda info # 创建',
    owner: 'emacsist',
    repo: 'emacsist.github.io',
    oauth: {
      client_id: 'd1456501fba5329f3afa',
      client_secret: 'd1ecbb7929a49de947215701320c60b312a72d3a'
    }
  })
  gitment.render('comments-gitment')
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://github.com/imsun/gitment">comments powered by gitment.</a></noscript>

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:emacsist@qq.com" class="iconfont icon-email" title="email"></a>
      <a href="https://twitter.com/emacsist2016" class="iconfont icon-twitter" title="twitter"></a>
      <a href="https://plus.google.com/u/0/114200054463267049438" class="iconfont icon-google" title="google"></a>
      <a href="https://github.com/emacsist" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/emacist" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/emacsist/" class="iconfont icon-douban" title="douban"></a>
  <a href="https://emacsist.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>


  <span class="copyright-year">
    &copy; 
    
      2014 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">emacsist</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  
<script type="text/javascript" src="/dist/even.min.js?v=3.1.1"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-118327923-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>









<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? "https://" : "http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1278300546'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s4.cnzz.com/z_stat.php%3Fid%3D1278300546%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</body>
</html>
